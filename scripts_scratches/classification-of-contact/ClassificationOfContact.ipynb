{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as stl\n",
    "stl.use(\"ggplot\")\n",
    "import cv2 as cv\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageData = cv.capture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagenerator = ImageDataGenerator(\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    rescale=1./255, # Nomalization of data. Chose 255 because we code each pixel by rgba\n",
    "    rotation_range=40, # Rotation to gain generalization\n",
    "    width_shift_range=0.2, \n",
    "    height_shift_range=0.2, \n",
    "    shear_range=.2, \n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True, \n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2)\n",
    "\n",
    "# Generates batches for training (120 x 120 tiles)\n",
    "train_generator = train_datagenerator.flow_from_dataframe(\n",
    "    imageData, \n",
    "    x_col='path', \n",
    "    y_col='type',\n",
    "    target_size=(120, 120), \n",
    "    color_mode='rgba', \n",
    "    class_mode='categorical', \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    seed=5201314,\n",
    "    subset='training')\n",
    "\n",
    "# Generates batches for testing (120 x 120 tiles)\n",
    "validation_generator = train_datagenerator.flow_from_dataframe(\n",
    "    imageData, \n",
    "    x_col='path',\n",
    "    y_col='type',\n",
    "    target_size=(120, 120),\n",
    "    color_mode='rgba', \n",
    "    class_mode='categorical', \n",
    "    batch_size=4, \n",
    "    shuffle=True,\n",
    "    seed=5201314, \n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "model = ks.models.Sequential()\n",
    "\n",
    "model.add(ks.layers.Dense(4, input_shape=(120, 120, 4)))\n",
    "\n",
    "# Add first convolutional layer \n",
    "model.add(ks.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(ks.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "# Add second convolutional layer\n",
    "model.add(ks.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(ks.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "# Add second convolutional layer\n",
    "model.add(ks.layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(ks.layers.MaxPooling2D(2, 2))\n",
    "model.add(ks.layers.Dropout(0.2))\n",
    "\n",
    "# Flattens the imagesfrom high dimensions to low dimenions\n",
    "model.add(ks.layers.Flatten())\n",
    "        \n",
    "model.add(ks.layers.Dense(1024, activation='relu'))\n",
    "\n",
    "model.add(ks.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator, \n",
    "    epochs=95\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "699f0d5608a67a19c7001870c9ddbe2a02cb28825dbc0a4b5a7b9914fd4653ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
